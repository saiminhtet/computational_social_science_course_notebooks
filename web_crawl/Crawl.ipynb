{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Crawling\n",
    "\n",
    "Web crawlers are bots that travel around sites and gather designated web contents. This is a technology used by search engines like Google and in mass scale requires a data base for storing the information the crawler fetches. \n",
    " \n",
    "Search engines like Google, have massive crawling operations that have a good proportion of the world wide web traffic. The server load from crawling is heavily influenced by the amount of the traffic the server faces from other sources. \n",
    "\n",
    "Crawling a site is allowed by default and it is the job of the system admin to withdraw this permission. These restrictions can be  put in place by using specifications in a configuration file. An example of this would be the [robot.txt](https://github.com/robots.txt) file outlining the specification for crawling Github.  \n",
    "\n",
    "## Crawling with Scrapy\n",
    "\n",
    "The first step is to create a project in a directory of your choice. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
